# LCPO：用于推理链长度控制的训练方法

## 任务背景

在复杂推理、数学求解和代码生成等任务中，增加测试时间计算（即生成更长的思维链）可以显著提升模型性能。然而，现有的方法缺乏对推理链长度的**精确与动态控制**，导致无法在性能与效率之间取得理想平衡。

---

## 研究难点

1. **如何让模型精确控制输出长度？**
2. **在满足指定长度限制的同时保持准确性？**
3. **如何训练模型在推理时根据任务动态调整计算资源？**

---

## 方法概述：LCPO（Length-Controlled Policy Optimization）

### 设计动机

观察到模型通过生成更长的思维链可提升性能，但其推理长度不可控。因此提出 **LCPO**，一种专门用于训练推理专用模型以实现**精确且自适应长度控制**的方法。

### 核心思想

- 在训练过程中，从预设范围内均匀采样目标长度 $ n_{gold} $。
- 将该长度作为提示的一部分输入模型。
- 使用一个奖励函数评估模型表现：
  - 包括答案正确性奖励；
  - 减去长度偏差项（实际 token 数与目标 token 数之差乘以系数 $ \alpha $）。
- 使用 **GRPO（Generalized Reinforcement Policy Optimization）算法** 训练模型，最大化累积奖励。

### 奖励函数设计

奖励函数具有双重目的：

1. 鼓励生成正确的答案；
2. 在指定较短输出时隐含地倾向于简洁推理；
3. 激励模型始终匹配规定的目标长度，即使可用更少 token 得出正确结果。

我们将以此目标训练的模型称为 **L1-Exact**。

---

##论文实验

## 数据集

### 训练数据：DeepScaleR-Preview-Dataset

- 来源：AIME、AMC、Omni-Math 和 STILL 提取的 **40,000 个问题-答案对**。
- 特点：专注于数学推理问题，用于训练模型在数学领域的推理能力。
- 目标：模型需学习生成正确答案，并满足特定长度约束。

### 测试数据（共6个测试集）

| 名称 | 描述 | 类型 |
|------|------|------|
| AIME 2025 | 美国数学邀请赛 2025 年试题 | 数学 |
| MATH (Hendrycks et al., 2021b) | 数学问题解决基准 | 数学 |
| AMC | 美国数学竞赛测试集 | 数学 |
| Olympiad-Bench (He et al., 2024) | 奥林匹克级别科学问题 | 科学 |
| GPQA (Rein et al., 2023) | 研究生级问答基准 | 综合 |
| LSAT (Zhong et al., 2023) | 法学院入学考试逻辑题 | 逻辑 |
| MMLU (Hendrycks et al., 2021a) | 多任务语言理解基准 | 知识 |

> 测试目标：评估模型在不同长度约束下的性能及其在未见过任务上的泛化能力。

---

## 模型架构

| 模型名称 | 描述 |
|----------|------|
| DeepSeek-R1-Distill-Qwen-1.5B | 经过 R1 推理痕迹微调 |
| DeepScaleR-1.5B-Preview | 原始模型，未进行长度控制修改 |
| DeepScaleR-1.5B-Preview-4K | 使用 4K 上下文长度微调的 Agentic-24K 版本 |

---

## 对比方法

- **S1（Muennighoff et al., 2025）**：预算强制方法，使用简单干预控制推理长度。

---

## 评价指标

1. **平均长度偏差**：$ n_y $（实际生成 token 数）与 $ n_{gold} $（目标长度）之间的平均差异。
2. **准确率（解决问题）**：在不同目标长度下模型的总体性能。
3. **目标长度集合**：{512, 1024, 2048, 3600} tokens。

---

## 实现细节

- **GRPO 超参数**：与 DeepScaleR-1.5B Preview 相同。
- **学习率**：1e-6
- **Batch Size**：128
- **上下文长度**：训练时为 4k tokens，评估时扩展至 8k tokens。
- **训练框架**：VeRL (MLSys, 2025)
- **训练步数**：700 步
- **目标长度采样范围**：$ U(n_{min}, n_{max}) $，其中 $ n_{min}=100 $, $ n_{max}=4000 $
- **平衡参数 $ \alpha $**：固定为 0.0003

> 注意：未进行广泛超参数搜索，预期可通过进一步优化提升性能。

---

## 模型类型

| 类型 | 描述 |
|------|------|
| L1-Exact | 要求生成的推理链长度**恰好等于**目标长度 |
| L1-Max | 要求生成的推理链长度**不超过**目标长度（动态调整） |

---

## 实验目标

1. **验证长度控制有效性**  
   - 检查 L1-Exact 和 L1-Max 是否能严格遵循用户指定的长度约束，从而实现推理成本与性能的灵活权衡。

2. **评估性能与长度关系**  
   - 分析不同长度约束下模型准确率变化趋势；
   - 与现有方法（如 S1）对比优势。

3. **探索泛化能力**  
   - 评估模型在训练数据之外的任务（如逻辑推理、知识理解）上的表现；
   - 验证是否能将长度控制能力迁移到其他领域。

4. **研究短推理链性能**  
   - 探讨 LCPO 在 Short-CoT 场景下的表现；
   - 检查在有限 token 预算下是否仍能高效推理。

