# CrossFormer 技术报告：原理、实现与未来展望

**摘要：** 现代机器学习系统依赖大规模数据集来实现广泛的泛化能力，这在机器人学习领域通常是一个挑战，因为每个机器人平台和任务可能只有小规模数据集。通过在多种不同类型的机器人上训练单一策略，机器人学习方法可以利用更广泛、更多样化的数据集，从而提升泛化性和鲁棒性。然而，在多机器人数据上训练单一策略具有挑战性，因为机器人可能拥有差异巨大的传感器、执行器和控制频率。CrossFormer 是一种可扩展且灵活的基于 Transformer 的策略，旨在解决这一挑战，能够处理来自任何机器人本体的数据。该模型在迄今为止最大、最多样化的数据集上进行了训练，包含来自20种不同机器人本体的90万条轨迹。实验证明，相同的网络权重可以控制截然不同的机器人，包括单臂和双臂操作、轮式机器人、四旋翼飞行器和四足机器人。与先前工作不同，CrossFormer 不需要手动对齐观察或动作空间。大量的真实世界实验表明，该方法能够匹敌为每个机器人本体定制的专家策略的性能，并显著优于先前最先进的跨本体学习方法。

## 1. 引言

机器人学习领域的一个核心挑战是如何开发出能够泛化到不同任务、环境和机器人硬件的策略。传统方法通常为每个特定的机器人或任务训练单独的策略，这限制了数据的复用，并使得在新机器人或任务上部署时需要大量重新训练。随着大规模预训练模型在自然语言处理和计算机视觉等领域取得巨大成功，研究者们开始探索如何将类似的思想应用于机器人学，旨在构建能够从多样化经验中学习的“机器人基础模型”。

然而，机器人领域的数据具有高度异构性。不同的机器人拥有不同的传感器套件（例如，不同数量和类型的摄像头、有无本体感受传感器）、不同的动作空间（例如，关节控制、末端执行器姿态控制、速度指令），以及不同的控制频率。这种异构性使得训练一个统一的策略来控制多种机器人变得非常困难。

CrossFormer ([Doshi et al., 2024](https://arxiv.org/abs/2408.11812)) 提出了一种解决方案，旨在创建一个单一的、可扩展的、灵活的基于 Transformer 的策略，该策略能够有效地利用来自不同机器人本体的大规模多样化数据集，而无需手动对齐观察或动作空间。其核心目标是实现一个“通用”机器人控制器，能够操作包括机械臂、导航机器人、四足机器人和飞行器在内的多种机器人平台。

## 2. CrossFormer：技术原理与架构

CrossFormer 的核心是一种基于 Transformer 的策略，它将机器人学习问题建模为一个序列到序列的转换任务。该模型能够处理来自不同机器人本体的、具有可变维度和模态的观察数据，并为它们生成合适的动作指令。

### 2.1. 整体架构

CrossFormer 采用了一个共享的、因果的（causal）、仅解码器（decoder-only）的 Transformer 主干网络。其设计关键在于如何灵活地处理不同机器人本体的输入和输出。

* **Transformer 主干网络：** 这是模型的核心，负责处理序列化的输入数据并生成用于动作预测的表示。所有机器人本体共享相同的 Transformer 权重，从而促进知识迁移。
* **处理多样化观察和动作：** CrossFormer 的一个关键创新在于它不需要手动对齐不同机器人的观察空间或动作空间。它通过将所有输入（观察、任务指令）标记化并组织成一个扁平的序列来实现对可变输入的处理。对于动作，它通过在输入序列中插入“动作读出标记”（action readout tokens），并将相应的输出嵌入传递到特定于动作空间的头部，以生成正确维度的动作向量。
* **模态特定标记器（Modality-Specific Tokenizers）：** 原始的观察数据（如图像、本体感受信息）和任务规范（如语言指令、目标图像）首先由特定于其模态的标记器处理，转换成 Transformer 能够理解的标记（tokens）。
* **输入序列组装：** 经过标记化的观察和任务规范被组装成一个单一的、扁平的输入标记序列。这种序列化对于处理来自不同机器人的可变数量的摄像头视图或本体感受传感器至关重要。
* **动作头（Action Heads）：** Transformer 的输出嵌入被送入独立的动作头。每个动作头特定于某一类机器人本体，并负责为该机器人生成正确维度和类型的动作。

**图1：CrossFormer 策略架构 (改编自 [Doshi et al., 2024, Figure 2])**
```
+-------------------------+      +-------------------------+      +-------------------------+
|   Observation Source 1  |      |   Observation Source 2  | ...  |   Observation Source N  |
| (e.g., Workspace Image) |      | (e.g., Proprioception)  |      | (e.g., Language Goal)   |
+-------------------------+      +-------------------------+      +-------------------------+
            |                              |                                |
            v                              v                                v
+-------------------------+      +-------------------------+      +-------------------------+
| Modality-Specific Tokenizer 1 | | Modality-Specific Tokenizer 2 | | Modality-Specific Tokenizer N |
+-------------------------+      +-------------------------+      +-------------------------+
            |                              |                                |
            +------------------------------+--------------------------------+
                                           |
                                           v
+-----------------------------------------------------------------------------------+
|                     Input Token Sequence (Flattened and Assembled)                  |
| [Obs_t_src1, Obs_t_src2, ..., Task_t, Readout_t, Obs_t+1_src1, ..., Readout_t+1, ...] |
+-----------------------------------------------------------------------------------+
                                           |
                                           v
+-----------------------------------------------------------------------------------+
|                               Transformer Backbone                                  |
|                      (Causal, Decoder-Only, Shared Weights)                       |
+-----------------------------------------------------------------------------------+
                                           |
                                           v (Embeddings for Readout Tokens)
+-----------------------------------------------------------------------------------+
|                Action-Space Specific Heads (Per Embodiment Class)                 |
| +-----------------+   +-----------------+   +-----------------+   +-----------------+ |
| | Head 1 (e.g.,   |   | Head 2 (e.g.,   |   | Head 3 (e.g.,   |   | Head 4 (e.g.,   | |
| | Single Arm)     |   | Navigation)     |   | Bimanual)       |   | Quadruped)      | |
| +-----------------+   +-----------------+   +-----------------+   +-----------------+ |
+-----------------------------------------------------------------------------------+
        |                      |                      |                      |
        v                      v                      v                      v
  Action Output 1      Action Output 2      Action Output 3      Action Output 4
 (Correct Dimension)  (Correct Dimension)  (Correct Dimension)  (Correct Dimension)
```

### 2.2. 训练数据

CrossFormer 在一个大规模且高度多样化的数据集上进行训练，这是其实现广泛泛化能力的关键。

* **数据集规模与多样性：** 训练数据包含 **90万条轨迹**，横跨 **20种不同的机器人本体**。这些机器人包括单臂和双臂操作平台、轮式导航机器人、四旋翼飞行器和四足机器人。
* **关键数据集：** 数据集的核心部分源自 Open Cross-Embodiment dataset (OXE) 的单臂操作子集。此外，还包括：
    * DROID Franka 操作数据集
    * ALOHA-multi-task：包含7000条ALOHA双臂操作轨迹
    * GNM 数据集：包含60小时的导航数据
    * Go1-walk：包含Unitree Go1四足机器人25分钟的行走数据
    * Franka-tabletop：作者实验室收集的200条额外Franka单臂操作轨迹
* **数据组成与采样权重：** 为了平衡不同类型机器人的数据，训练批次中的数据按特定比例采样。平均而言，每个训练批次的数据构成为：单臂操作 (58%)，双臂操作 (17%)，导航 (17%)，四足 (8%)。特定的“目标数据集”（如BridgeData之于WidowX评估）在训练时会被上采样以优化在特定评估设置上的性能。详细的各数据集采样权重见原论文附录A。

### 2.3. 观察标记化 (Observation Tokenization)

CrossFormer 能够处理多种类型的观察输入，并将它们转换为 Transformer 主干网络可以处理的统一标记序列。

* **支持的观察类型：**
    * **工作区图像 (Workspace Image)：** 通常用于操作任务的第三方视角摄像头图像。
    * **导航图像 (Navigation Image)：** 用于导航任务的机器人自我中心视角图像。
    * **腕部图像 (Wrist Image)：** 安装在机械臂腕部的摄像头图像，也用于操作任务。
    * **四足本体感受 (Quadruped Proprioception)：** 四足机器人的关节位置和速度估计。
    * **双臂本体感受 (Bimanual Proprioception)：** 双臂操作平台的关节位置。
* **图像标记化：**
    * 图像首先通过一个 **ResNet-26 编码器** 进行处理，生成特征图。
    * 该特征图在空间维度上被展平。
    * 然后通过线性投影将其转换为 Transformer 的标记嵌入大小。
* **本体感受标记化：**
    * 本体感受观察（如关节位置、速度估计）被直接线性投影到标记嵌入大小。
* **任务规范 (Task Specification)：** 策略可以接受两种形式的任务规范：
    * **语言指令：** 语言指令与图像观察一起使用 **FiLM (Feature-wise Linear Modulation)** 进行处理。
    * **目标图像：** 目标图像在输入图像编码器之前，在通道维度上与当前图像堆叠。
* **图像编码器权重共享：** 为了最大化跨不同机器人本体的知识迁移，相同类型的摄像头视图共享图像编码器权重。例如，单臂和双臂操作设置中的工作区图像都由同一个ResNet图像编码器处理。总共使用了四个图像编码器：一个用于工作区视图，一个用于自我中心导航视图，两个用于腕部摄像头。
* **处理缺失观察：** 对于特定机器人本体缺失的观察类型，在实际操作中会被掩码（masked），以确保每个训练批次包含所有观察类型和读出标记组，它们在上下文窗口中占据固定位置。

输入序列由长度为 `k` 的观察历史构成，每个时间步的观察被标记化后组装成如 $[I_{t}^{1:L}, P_{t}^{1:M}, ..., I_{t+k}^{1:L}, P_{t+k}^{1:M}]$ 的形式，其中 $I$ 代表图像标记，$P$ 代表本体感受标记，$L$ 和 $M$ 分别是它们的标记数量。

### 2.4. 动作预测 (Action Prediction)

CrossFormer 能够为不同机器人本体预测可变长度和维度的动作。

* **动作读出标记 (Action Readout Tokens, R)：** 在每个时间步的观察标记之后，特殊的“读出标记”被插入到输入标记序列中。这些读出标记被设计为仅关注（attend to）先前的观察标记，并作为预测动作的便捷表示。输入序列的结构变为 $[I_{t}^{1:L}, P_{t}^{1:M}, R_{t}^{1:N}, ..., I_{t+k}^{1:L}, P_{t+k}^{1:M}, R_{t+k}^{1:N}]$，其中 $N$ 是读出标记的数量。
* **动作空间特定的头部 (Action-Space Specific Heads)：** 输入标记序列通过 Transformer 后，对应于读出标记的输出嵌入被送入特定于该机器人动作空间的动作头，以生成最终的动作。这些头部通常是简单的线性投影层。
* **可变长度动作预测与动作分块 (Action Chunking)：** CrossFormer 可以预测可变大小的动作“块”（chunks of actions）。这种技术对于高控制频率的机器人（如精细操作任务）尤其重要，因为它能提高动作的时间一致性并减少累积误差。每个机器人本体的读出标记数量与其对应的动作块大小相匹配。
* **支持的动作类型与特性：** CrossFormer 包含四个不同的动作头，为不同类型的机器人生成分块动作：
    * **单臂笛卡尔位置：** 7维动作（末端执行器相对笛卡尔位置变化和夹爪驱动）。预测4个动作块，执行频率5-15Hz。
    * **导航路标点：** 2维动作（相对于机器人当前位置的路标点）。预测4个动作块，执行频率4Hz。
    * **双臂关节位置：** 14维动作（双臂的关节位置）。预测100个动作块，执行频率20Hz。
    * **四足关节位置：** 12维动作（腿部关节位置）。不进行分块（预测1个动作），执行频率20Hz。
* **损失函数：** 策略预测连续动作，并使用 L1 损失函数进行训练，这在先前的高频双臂操作研究中被证明是有效的。

### 2.5. 实现细节

* **模型参数：** 包括ResNet-26图像编码器和动作头在内，整个模型拥有 **1.3亿 (130M)** 参数。
    * Transformer 层数：12层
    * 注意力头数：8个
    * 标记嵌入大小 (Token Embedding Size)：512
    * MLP 维度 (MLP Dimension)：2048
* **上下文长度 (Context Length)：** 上下文窗口大小为 **2135个标记**，这允许在所有观察和读出标记组都存在的情况下，容纳5个时间步的上下文。

### 2.6. 训练过程

* **优化器：** 使用 AdamW 优化器。
* **学习率：** 初始学习率为 $3 \times 10^{-4}$。
* **学习率调度器：** 采用反平方根学习率调度策略，包含2000个预热步骤 (warmup steps)。
* **权重衰减 (Weight Decay)：** 0.1。
* **梯度裁剪 (Gradient Clipping)：** 阈值为1.0。
* **批次大小 (Batch Size)：** 512。
* **总训练步数：** 30万次梯度步。
* **训练时间与硬件：** 在TPU V5e-256 Pod上训练耗时约47小时 (原论文正文) 或80小时 (原论文附录B)。
* **初始化：** ResNet-26 编码器使用 ImageNet 预训练权重进行初始化。
* **数据增强：** 训练期间应用了标准的图像增强方法，特别是颜色抖动 (color jitter) 和随机调整大小/裁剪 (random resizing/cropping)。
* **事后目标重标签 (Hindsight Goal Relabeling)：** 使用了事后目标重标签技术，并从未来观察中均匀随机采样作为目标。
* **任务规范掩码：** 如果轨迹有可用的语言指令，则随机掩盖语言指令或目标图像。这使得策略在测试时可以根据任一任务规范进行条件化。
* **检查点选择：** 根据验证均方误差选择模型和基线的检查点，因为发现这与机器人性能大致相关。

## 3. 评估与性能

CrossFormer 在多种真实机器人和任务上进行了广泛评估，以验证其有效性。

### 3.1. 评估设置

评估旨在回答以下关键问题：
1.  CrossFormer 能否匹敌仅在目标机器人数据上训练的策略的性能？
2.  CrossFormer 能否匹敌每个机器人设置中先前最佳模仿学习方法的性能？
3.  与先前对齐观察和动作空间的跨本体策略相比，CrossFormer 表现如何？

评估在以下机器人本体和任务上进行：
* **WidowX 操作：**
    * 设置：使用 Bridge 设置。
    * 视图：过肩摄像头视图。
    * 任务：语言条件（“将勺子放在布上”，“将胡萝卜放在盘子上”）和目标条件（“将蘑菇放入锅中”，“将布放在碟子上”）。
* **Franka 操作：**
    * 设置：使用 DROID 设置。
    * 视图：过肩摄像头视图。
    * 任务：语言条件（“用海绵将松果扫入簸箕”，“将锅翻正”）。
* **ALOHA 双臂操作：**
    * 设置：使用 ALOHA 设置。
    * 视图：三个摄像头视图（一个头顶，两个腕部）。
    * 任务：语言条件（“取下笔帽”，“拿起刀切寿司”）。
* **LoCoBot 导航：**
    * 设置：使用 LoCoBot 设置。
    * 视图：一个摄像头视图。
    * 技能：路径跟随、避障、急转弯。
* **Go1 四足机器人：**
    * 设置：Unitree Go1。
    * 观察：本体感受。
    * 任务：向前行走（直接控制关节）。
* **Tello 四旋翼飞行器：**
    * 设置：Tello。
    * 动作：使用导航头输出2D相对路标点，保持固定高度。
    * 泛化：零样本泛化到新本体（训练数据中无四旋翼数据）。
    * 任务：转弯。

### 3.2. 基线模型

CrossFormer 与以下基线进行了比较：
* **单一机器人数据集基线 (Single-Robot Dataset Baseline)：** 使用 CrossFormer 的架构，但仅在目标机器人本体的数据上进行训练。为避免在小数据集上过拟合，使用了较小版本的架构 (5M-95M 参数)。
* **先前最佳方法 (Best Prior Method)：** 针对每种机器人调整的最佳现有模仿学习方法：
    * WidowX: Octo
    * Franka: OpenVLA
    * ALOHA: ACT
    * LoCoBot 和 Tello: VINT
    * Go1: 无合适的先前模仿学习方法。
* **Yang et al. (2023) 策略：** 一种对齐导航和操作任务的观察和动作空间的策略。

### 3.3. 主要性能结果

* **与单一机器人数据集基线比较：**
    * CrossFormer 在所有评估中表现与仅在目标数据集上训练的策略相当，表明从差异巨大的机器人本体吸收数据并未导致负迁移。
    * 在所有机器人本体上的平均成功率为：CrossFormer **73%** vs. 单一机器人基线 **67%**。

* **与先前最佳方法比较：**
    * CrossFormer 在每个评估设置中表现与先前最佳模仿学习方法相似，展示了其匹敌为单个机器人专门开发和调整的SOTA方法的能力。
    * 在所有机器人本体上的平均成功率为：CrossFormer **73%** vs. 先前最佳方法 **51%** (Go1被排除在此平均值之外，因无合适的先前方法)。

* **与 Yang et al. 比较：**
    * CrossFormer 的整体性能显著优于 Yang et al. (约3倍)。
    * 在LoCoBot导航任务中，CrossFormer 在所有场景均大幅优于 Yang et al.，表现出更平滑的导航。
    * 在WidowX操作任务中，Yang et al. 成功率为零，而CrossFormer表现良好。作者推测这可能是因为 Yang et al. 一次只输入一个摄像头视图，可能导致对第三方视角控制的欠拟合。CrossFormer的灵活架构允许同时接受多个摄像头视图。
    * 这些结果表明，手动对齐观察和动作空间对于良好性能并非必需，CrossFormer的灵活架构使其能更好地适应异构的跨本体数据。

**详细评估结果摘要 (改编自 [Doshi et al., 2024, Table 3])**

| 任务类别 (代表性任务)          | 单一机器人数据集 | 先前最佳方法 | CrossFormer |
| :------------------------------- | :--------------- | :----------- | :---------- |
| **WidowX 操作** (平均成功率)     | 0.42             | 0.34         | 0.40        |
|  *放勺子到布上* | 0.25             | 0.25 (Octo)  | 0.25        |
|  *放蘑菇到锅里* | 0.00             | 0.17 (Octo)  | 0.25        |
| **Franka 操作** (平均成功率)       | 0.55             | 0.57         | 0.55        |
|  *用海绵扫松果* | 0.41             | 0.52 (OpenVLA)| 0.41        |
| **ALOHA 双臂** (平均成功率)        | 0.50             | 0.50         | 0.70        |
|  *取下笔帽* | 0.60             | 0.70 (ACT)   | 0.80        |
| **LoCoBot 导航** (平均到达子目标比例)| 0.92             | 0.48         | 0.93        |
|  *避障* | 0.95             | 0.30 (VINT)  | 0.95        |
| **Tello 四旋翼** (平均到达子目标比例)| 0.68             | 0.68 (VINT)  | 0.82        |
| **Go1 四足** (标准化奖励)          | 1.00             | N/A          | 1.00        |
| **总体平均 (不含 Go1 先前方法)** | **0.68** | **0.51** | **0.73** |

*(注：上表为简化摘要，具体任务和试验次数请参考原论文)*

## 4. 局限性与未来方向 (源自CrossFormer论文)

尽管 CrossFormer 取得了显著进展，但作者也指出了其存在的一些局限性，并提出了未来的研究方向。

### 4.1. 局限性

* **对未见过的机器人本体的泛化：** 虽然 CrossFormer 能处理训练中见过的多种机器人，但其对全新、形态迥异的机器人本体的零样本泛化能力仍有待探索。
* **对新动作/观察空间的数据需求：** 当引入具有全新动作或观察空间的机器人时，模型可能仍需要一些领域内数据进行微调或适应。
* **样本效率：** 虽然利用了大规模数据集，但进一步提高样本效率，特别是对于新本体或新任务的适应，仍然是一个挑战。
* **可解释性：** 与许多基于 Transformer 的大规模模型类似，CrossFormer 内部决策过程的可解释性有待提高。
* **静态世界假设：** 当前的评估主要在相对可控的环境中进行，对动态、非结构化环境的鲁棒性需要进一步研究。
* **单个本体内的多任务学习：** 虽然模型是跨本体的，但在单个机器人本体上执行大量语义不同任务的能力可以进一步增强。

### 4.2. 未来研究方向 (作者建议)

* **提升对全新机器人的泛化能力：** 研究如何使模型能更好地零样本或少样本泛化到训练中未曾见过的机器人本体，可能通过更抽象的机器人表示或元学习方法。
* **减少对新本体的数据需求：** 开发更高效的迁移学习和领域自适应技术，以最小化引入新机器人时所需的数据量。
* **与语言模型集成以实现复杂推理/规划：** 将 CrossFormer 与大规模语言模型 (LLMs) 结合，以实现更高级的指令理解、任务规划和场景推理能力。
* **整合触觉感知：** 将触觉等其他重要感知模态融入模型，以增强操作任务的灵巧性和鲁棒性。
* **与强化学习结合：** 探索将模仿学习与强化学习相结合的范式，利用模仿学习进行初始化，再通过强化学习进行微调和探索，以学习更复杂或数据稀疏的任务。

## 5. 进一步的研究与发展

CrossFormer 的提出正值机器人基础模型研究兴起之时。自其发布以来（arXiv预印本2024年8月），相关领域持续活跃，研究者们从不同角度探索如何构建更通用、更强大的机器人智能体。

### 5.1. Shadow: Leveraging Segmentation Masks for Cross-Embodiment Policy Transfer

* **作者与会议：** Lepert, Doshi, Bohg, CoRL 2024. (Ria Doshi 是 CrossFormer 的共同作者)
* **核心思想：** "Shadow" 提出了一种数据编辑方案，利用分割掩码来实现机器人操作策略到未见过的机器人手臂的零样本视觉迁移。其关键在于通过分割出与任务相关的物体（如工具、目标物体），使得策略能够专注于这些关键元素，而忽略机器人手臂本身的视觉外观差异。这使得在仅有源机器人数据的情况下，策略也能应用于目标机器人，只要分割掩码能够可靠地提取。
* **与CrossFormer的关系：** "Shadow" 的研究方向与 CrossFormer 具有互补性。CrossFormer 旨在构建一个能够处理多种传感器和动作空间的统一架构，并在大规模多样化数据上训练。而 "Shadow" 则专注于解决特定场景下的视觉跨本体迁移问题，特别是针对机械臂操作，通过显式地利用分割信息来提升零样本迁移性能。这种方法可以被视为一种增强跨本体泛化能力的特定技术，有可能与 CrossFormer 这样的通用架构相结合。

### 5.2. Universal Actions for Enhanced Embodied Foundation Models (UniAct)

* **作者与会议：** Zheng et al., CVPR 2025 (arXiv:2501.10105).
* **核心思想：** 该研究针对机器人基础模型中动作空间异构性带来的挑战，提出了一个名为 `UniAct` 的新框架。`UniAct` 的核心是在一个“通用动作空间”（Universal Action Space）中操作。它通过利用不同机器人动作之间的共享结构特征，学习能够捕获跨多种机器人普适原子行为的“通用动作”。这些通用动作随后可以被翻译回特定机器人可执行的异构指令，从而实现对新机器人的快速适应。
* **与CrossFormer的关系：** `UniAct` 与 CrossFormer 同样致力于解决跨机器人本体学习中的动作异构性问题。然而，它们采取了不同的策略。CrossFormer 通过为每类机器人本体配备特定的动作头，并在训练中让模型自行学习如何从共享表示映射到特定动作空间，从而避免了对动作空间的显式对齐。相比之下，`UniAct` 试图定义并学习一个中间的、通用的动作表示。这两种方法代表了处理动作异构性的不同思路：CrossFormer 强调架构的灵活性和隐式学习，而 `UniAct` 则探索显式的通用动作抽象。

### 5.3. 更广阔的背景与启示

CrossFormer 的工作，连同 "Shadow" 和 "UniAct" 等相关研究，共同描绘了机器人学习领域向更通用、更大数据驱动模型发展的趋势。这些研究表明，通过精心设计的架构、大规模多样化的数据以及创新的表征学习方法，有可能克服机器人硬件和任务的异构性，构建出能够适应多种场景的机器人智能体。未来的研究可能会继续探索如何更有效地融合来自不同来源的数据，如何实现更鲁棒的零样本/少样本泛化，以及如何将这些模型与高级认知能力（如长期规划、复杂推理）相结合。

## 6. 结论

CrossFormer 作为一种可扩展且灵活的跨本体机器人策略，在推动机器人学习向通用化方向发展方面迈出了重要一步。其核心贡献在于提出了一种无需手动对齐观察和动作空间的 Transformer 架构，能够有效利用来自多种机器人（包括操作、导航、移动和飞行平台）的大规模数据集进行训练，并在真实世界的多种任务中展现出与专家策略相当的性能。

通过巧妙的标记化方案、动作读出机制和特定于本体的动作头，CrossFormer 成功地用一套共享网络权重控制了形态和功能迥异的机器人。这不仅验证了构建通用机器人控制器的可行性，也为未来机器人基础模型的研究奠定了坚实基础。尽管存在一些局限性，如对全新本体的泛化能力和样本效率等，但 CrossFormer 及其揭示的未来研究方向，无疑将持续激励该领域向着更强大、更普适的机器人智能不断迈进。

