

# GR-2 技术报告：一种具备网络规模知识的生成式视频-语言-动作大模型用于机器人操作

**摘要**：GR-2 是一个先进的通用机器人代理，专为多样化和可泛化的机器人操作而设计。GR-2 首先在海量互联网视频上进行预训练，以捕捉世界的动态。这种大规模预训练（涉及3800万个视频剪辑和超过500亿个token）使 GR-2 能够在后续的策略学习中泛化到广泛的机器人任务和环境。随后，GR-2 使用机器人轨迹数据针对视频生成和动作预测进行微调。它展现了令人印象深刻的多任务学习能力，在超过100个任务中平均成功率达到97.7%。此外，GR-2 表现出卓越的泛化能力，能够适应新的、以前未见过的场景，包括新颖的背景、环境、物体和任务。值得注意的是，GR-2 能够有效地随模型规模扩展，突显了其持续增长和应用的潜力。

**项目页面**: <https://gr2-manipulation.github.io>
**原始论文**: Cheang, C.-L., Chen, G., Jing, Y., Kong, T., Li, H., Li, Y., Liu, Y., Wu, H., Xu, J., Yang, Y., Zhang, H., & Zhu, M. (2024). *GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation*. arXiv:2410.06158 [cs.RO].

## 1. GR-2 技术原理与实现细节

### 1.1. 整体架构与两阶段训练

GR-2 模型是一个以语言为条件的 GPT 风格的视觉操作策略模型，其训练过程分为两个核心阶段：

**阶段一：视频生成预训练 (Video Generative Pre-training)**

* **目标**：此阶段旨在让模型预测未来的视频帧。通过掌握这项任务，模型能够为预测未来事件建立强大的先验知识，从而增强其进行准确动作预测的能力。这使得模型能够捕捉对下游策略学习至关重要的关键时间动态和语义信息。
* **输入**：模型接收标记化的文本和图像序列作为输入。
    * **语言指令**：使用一个冻结的文本编码器（例如，参考论文中的[6]）对语言指令进行标记化处理。
    * **图像帧**：视频中的图像帧通过一个预训练的 VQGAN (Vector Quantized Generative Adversarial Network) [7] 转换为离散的标记。这个 VQGAN 在大量的互联网数据和领域内机器人数据上训练，并在 GR-2 的训练过程中保持冻结，以促进快速训练和支持高质量视频的生成。
    * **机器人状态** (在微调阶段更明确使用，但预训练数据可包含相关视觉信息)：机器人状态，如末端执行器的位置和旋转以及夹爪状态，在微调阶段通过可训练的线性层进行编码。
* **模型架构**：GR-2 建立在一个 GPT 风格的 Transformer 之上。
* **输出**：模型输出未来图像的离散标记，这些标记随后通过 VQGAN 解码器解码为实际的未来图像。
* **数据规模与处理**：
    * GR-2 在远超以往工作的数据量上进行预训练，使用了**3800万个文本-视频片段**，相当于约**500亿个token**。
    * 数据集来源广泛，包括常用的人类活动公共数据集，如 Howto100M, Ego4D, Something-Something V2, EPIC-KITCHENS 和 Kinetics-700。图2展示了人类活动和视频样本的分布。
    * 为了使预训练数据更适合机器人操作任务，研究团队精心建立了一个数据处理流程，包括**手动筛选 (hand filtering)** 和**重新字幕 (re-captioning)**。
    * 此外，还整合了公开可用的机器人数据集，如 RT-1 和 Bridge 数据集。

**阶段二：机器人数据微调 (Robot Data Fine-tuning)**

* **目标**：在大规模视频预训练之后，GR-2 模型无缝地迁移到机器人数据上进行微调。此阶段的目标是**联合预测动作轨迹和视频**。
* **输入**：
    * **语言指令**：与预训练阶段类似，使用冻结的文本编码器进行标记化。
    * **多视角视频帧序列**：与预训练数据中通常仅包含单一摄像机视角的视频不同，机器人数据通常包含多个视角（例如，头部摄像头和末端执行器摄像头）。GR-2 被设计为能够优雅地处理这种多视角输入。图像帧同样通过 VQGAN 转换为离散token。
    * **机器人状态序列**：包括末端执行器的位置和旋转，以及二值化的夹爪状态。这些状态通过在微调阶段可训练的线性层进行编码。
* **输出**：
    * **各视角的未来图像**：模型为每个输入视角预测未来的图像帧。
    * **动作轨迹**：动作轨迹是使用**条件变分自编码器 (Conditional Variational Autoencoder, CVAE)** [17, 18, 19] 生成的。研究发现，生成完整的动作轨迹而非单步动作，对于保证轨迹的平滑性和实时性能至关重要。
* **数据增强**：为了提升模型对未见场景的泛化能力，微调阶段采用了数据增强技术：
    * **场景物体插入**：使用一个扩散模型 [22]（在自收集的物体数据集和 Open Images 数据集上训练）将新物体插入到场景中的指定区域。
    * **背景替换**：利用 Segment Anything Model (SAM) [24] 提取与背景对应的区域，然后使用一个视频生成模型（以原始视频和修复后的帧为条件）生成背景替换后的增强视频，同时保留机器人原有的运动。

图1直观地展示了GR-2的这一两阶段训练过程。首先进行“视频-语言预训练”，学习根据文本描述和视频帧预测后续帧。随后进入“视频-语言-动作微调”阶段，使用机器人轨迹数据进行训练，以执行具体的机器人操作任务。

### 1.2. GR-2 模型组件与 GR-1 的关键改进

**模型组件详情**：

* **GPT风格的 Transformer 架构**：作为模型的主干，处理标记化的文本、图像和机器人状态序列，自回归地预测未来。
* **VQGAN 编码器/解码器**：用于将图像帧与离散的视觉token进行相互转换。编码器将图像转换为token序列输入给Transformer，Transformer输出的视觉token再由解码器转换回图像帧。VQGAN 在训练过程中保持冻结。
* **冻结的文本编码器**：用于将语言指令转换为Transformer能够理解的token序列。
* **CVAE (条件变分自编码器)**：在微调阶段专门用于生成平滑且连贯的机器人动作轨迹。
* **线性层**：用于编码机器人状态信息。

**相较于 GR-1 的关键改进**：

GR-2 在其前身 GR-1 的基础上实现了多项显著的改进，从而带来了性能和能力的巨大提升：

1.  **显著扩大的预训练数据规模**：GR-2 的预训练数据量从 GR-1 的80万视频增加到**3800万个文本-视频数据**（超过500亿token）。这种规模的提升极大地增强了模型在各种未见场景中的泛化能力。
2.  **实现知识无损迁移的新模型架构**：GR-2 开发了一种新颖的模型架构，使得从预训练阶段获得的丰富知识能够无缝且无损地迁移到下游的机器人微调任务中。这种架构在处理多任务和应对挑战性泛化设置时表现出强大的可扩展性。
3.  **支持的任务数量和物体数量大幅增加**：GR-2 能够完成超过**100种操作任务**，并能对超过**100种物体**进行抓取，远超 GR-1。
4.  **高效学习能力**：GR-2 能够从仅包含约5000条轨迹（平均每个任务50条轨迹）的数据集中高效学习超过100个任务，显著降低了获取新技能和适应新环境的数据成本。
5.  **卓越的泛化能力**：GR-2 在泛化到新的、以前未见的场景方面表现出色，包括新的背景、新的环境、新的物体和新的任务。
6.  **模型规模的有效扩展**：实验证明 GR-2 能够有效地随着模型参数规模的增大而扩展其性能，预示着其通过持续增大模型和数据规模来进一步提升能力的潜力。

### 1.3. 真实机器人系统配置与部署

**系统配置** (参考第2.2节)：

GR-2 的真实机器人实验平台主要由以下组件构成：

* **机械臂**：1个7自由度 (7-DoF) 的 Kinova Gen3 机械臂。
* **夹爪**：1个 Robotiq 2F-85 并行夹爪，安装在机械臂末端。
* **摄像头**：
    * 1个静态的头部摄像头 (Static head camera)，提供工作空间的全局视图。
    * 1个安装在末端执行器上的摄像头 (Wrist camera)，提供操作区域的特写视图。

**部署方法与全身控制 (WBC)** (参考第2.2节)：

GR-2 模型生成的是笛卡尔空间中的动作轨迹。为了确保机械臂能够精确、平滑且安全地执行这些轨迹，研究团队开发并部署了一种**全身控制 (Whole-Body Control, WBC) 算法**。其主要功能包括：

1.  **轨迹优化**：模型生成的原始笛卡尔轨迹首先会经过优化，以提高其**平滑度 (smoothness)** 和**连续性 (continuity)**。
2.  **笛卡尔到关节空间的转换**：优化后的笛卡尔轨迹随后由 WBC 算法转换为机器人底层的关节动作指令。
3.  **高频控制**：这些关节动作指令以 **200 Hz** 的频率在真实机器人上执行。
4.  **集成约束**：WBC 算法在轨迹转换和执行过程中集成了重要的约束条件，包括：
    * **碰撞避免 (Collision constraints)**：确保机器人在运动过程中不会与自身或环境发生碰撞。
    * **可操作性 (Manipulability)**：确保机器人以有效和灵活的方式执行任务，避免奇异姿态等问题。

这种部署方法使得 GR-2 能够将其在高层学习到的策略有效地转化为真实世界中精确、流畅的机器人行为。

## 2. GR-2 实验结果与评估

### 2.1. 规模扩展实验 (Scaling Experiments)

(参考第3.5节和图11)
GR-2 研究了不同模型规模对其性能的影响，结果表明模型性能随着参数量的增加而提升。

* **模型规模**：评估了四种不同规模的GR-2模型（可训练参数）：
    * GR-2-S: 30M
    * GR-2-B: 95M
    * GR-2-L: 312M
    * GR-2-XL: 719M

* **预训练视频生成验证损失**：
    * 在 Ego4D、RT-1 和自定义机器人数据验证集上，随着模型规模从 S 增加到 XL，视频预测的验证损失均持续下降（图11a, b, c）。这表明更大的模型能更好地学习和预测视频帧。

* **真实机器人多任务学习成功率**：
    * 在真实机器人多任务实验中，模型的成功率也随着模型规模的增大而显著提升（图11d）。GR-2-XL (719M) 的成功率最高，达到了约70%，远高于较小规模的模型。这突显了通过增加模型规模来持续改进性能的潜力。

### 2.2. 多任务学习实验 (Multi-Task Learning Experiments)

(参考第3.1节, 图3, 图4, 图6)

* **任务设置与数据收集**：
    * GR-2 在 **105个桌面任务**上进行评估，这些任务涵盖了 **8种不同的操作技能**：拾取 (picking)、放置 (placing)、开盖 (uncapping)、合盖 (capping)、打开 (opening)、关闭 (closing)、按压 (pressing) 和倾倒 (pouring)。图4展示了任务示例。
    * 通过远程操作收集了约 **40,000条人类演示轨迹**，平均每个任务约400条轨迹。
    * 为了评估数据稀缺情况下的性能，还使用了约1/8的完整数据集（平均每任务约50条轨迹）进行训练。

* **详细评估设置** (图3)：
    * **基本设置 (Basic Settings)**：
        * **简单场景 (Simple)**：测试环境与训练数据相似。
        * **有干扰物场景 (Distractor)**：在简单场景基础上添加无关干扰物。
    * **泛化设置 (Generalization Settings)**：
        * **未见背景 (Unseen Backgrounds)**：使用与训练数据中不同的新桌布改变背景。
        * **未见环境 (Unseen Environments)**：在两个新的厨房环境中评估，这些环境不仅背景不同，还包含场景相关的干扰物。
        * **未见操作 (Unseen Manipulation)**：操作未见类别的物体和未见的物体实例。

* **性能与比较** (图6)：
    * **简单场景**：GR-2 取得了 **97.7%** 的高成功率。
    * **泛化能力**：
        * 未见背景：71.4%
        * 未见环境：71.7%
        * 未见操作：55.8%
    * **与 GR-1 比较**：GR-2 在所有设置下均显著优于 GR-1。特别是在“未见背景”和“未见环境”设置中，GR-2 的成功率是 GR-1 的两倍。
    * **数据增强效果 (GR-2 w/ DA)**：引入数据增强后，GR-2 (w/ DA) 在泛化设置中性能进一步提升，例如在“未见环境”中成功率达到 **87.0%**，三个泛化设置平均成功率为74.7%。
    * **少量数据训练 (Few-shot)**：即使仅用每任务约50条轨迹训练，GR-2 在简单场景下仍能达到 **73.9%** 的成功率，并且在所有三个泛化设置中表现优于使用全量数据训练的 GR-1，显示了其高效学习的潜力。

### 2.3. 端到端分拣实验 (End-to-End Bin Picking Experiments)

(参考第3.2节, 图7, 图8, 图9)

* **实验设置与语言指令**：
    * 设置包括一个源篮子和一个目标篮子（图7a）。训练时使用 **55种物体**，收集了约 **94,000条抓取-放置轨迹**。测试时共评估 **122种物体**，其中 **67种为训练时未见**（图7b）。
    * 语言指令为：“将任何物体从右边的篮子移动到左边的篮子。”

* **详细评估设置**：
    * **已见物体 (Seen)**：源篮子中包含5-9个训练时见过的物体。
    * **未见物体 (Unseen)**：源篮子中包含5-9个训练时未见的物体。
    * **杂乱场景中的已见物体 (Cluttered Seen)**：源篮子中包含12-17个（数量翻倍）训练时见过的物体。
    * **杂乱场景中的未见物体 (Cluttered Unseen)**：源篮子中包含12-17个（数量翻倍）训练时未见的物体。
    * 杂乱场景因物体数量远超训练时，本身即被视为未见过的场景配置。

* **性能与比较** (图8, 图9)：
    * GR-2 的平均成功率从 GR-1 的 33.3% 大幅提升至 **79.0%**。
    * GR-1 在处理未见物体和两种杂乱场景时性能急剧下降，几乎无法完成任务。
    * GR-2 在“未见物体”和两种“杂乱场景”下的成功率与“已见物体”场景下的成功率相当，展现了强大的泛化能力。
    * **处理挑战性物体**：GR-2 能够成功处理透明、可变形和反光等对传统基于模型的机器人方法具有挑战性的物体（图8展示了示例，如透明瓶子、可变形袋子和反光罐头）。

### 2.4. CALVIN 基准测试结果

(参考第3.3节, 图10)
CALVIN 是一个评估机器人在长序列、语言指令条件下的连续任务执行能力的基准。

* **GR-2 性能**：
    * 完成1个任务的成功率：**98.6%**
    * 连续完成5个任务的成功率：**85.9%**
    * 平均任务链长度 (Average Length)：**4.64** (衡量在被指示连续执行5个任务时，平均能完成的任务数量)

* **与SOTA方法对比**：
    * GR-2 在所有比较的基线方法（包括 RT-1, MT-ACT, HULC, RoboFlamingo, GR-1）中均取得了新的SOTA结果。
    * 相比 GR-1（成功率94.9% (1 task), 73.1% (5 tasks), 平均长度4.21），GR-2 在各项指标上均有显著提升。

### 2.5. 自回归视频生成能力

(参考第3.4节, 图12-17)
GR-2 的核心设计之一是其强大的自回归视频生成能力，这不仅是预训练的目标，也被认为是动作生成的有效规划器。

* **视频生成作为规划器**：模型通过预测图像空间中的未来状态（即生成未来的视频帧）来“规划”其动作。在生成视觉轨迹后，可以据此推断出相应的动作轨迹。这意味着模型通过预测“世界”将如何演变来指导其自身的行动。

* **生成视频与真实世界部署的一致性**：
    * 论文中的图12-17 展示了在多任务学习、端到端分拣和CALVIN基准测试等多种场景下，GR-2 生成的预测视频帧序列与真实机器人执行 rollout 的视频帧序列之间的比较。
    * 结果清晰地表明，GR-2 能够生成高质量的预测视频，并且这些生成的视频与真实世界的执行轨迹在视觉上高度一致。
    * 这种一致性支持了以下观点：模型预测的动作正在尝试“重放”其在预测视频中规划出的视觉轨迹。
    * 这一特性为通过迭代改进视频生成来持续改进动作预测提供了一种简单而有效的方法。

## 3. 基于 GR-2 的进一步研究与最新进展

截至2025年6月，对公开发表的学术文献进行的检索表明，尚无明确将 "GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation" (arXiv:2410.06158v1，发布于2024年10月8日) 作为直接引用或在其基础上进行显著扩展的后续研究工作公开发表。

可能的解释包括：
* **时间滞后**：学术研究从进行到成果发表通常需要一定的时间周期。
* **内部研发**：相关团队可能正在进行更深入的内部研究或准备更详尽的期刊/会议论文。
* **研究方向**：ByteDance Research 在大规模生成模型和基础模型方面持续活跃，但最新的、已发表的工作可能集中在其他应用领域，或者尚未直接关联回GR-2在机器人操作的具体框架上。

尽管如此，GR-2 所代表的通过大规模视频预训练赋予机器人通用操作能力的研究方向，是当前机器人学习领域的热点。预计未来会有更多研究工作借鉴或扩展类似 GR-2 的理念，例如：
* 探索更大规模、更多样化的预训练数据集。
* 研究更高效的模型架构和训练方法。
* 将此类模型应用于更复杂的任务和更动态的环境。
* 提升模型的可解释性、安全性和鲁棒性。

建议关注 GR-2 项目页面 (<https://gr2-manipulation.github.io>) 以及主要作者（如 Tao Kong, Hongtao Wu, Ya Jing 等）和 ByteDance Research 在机器人与人工智能领域的未来发表，以获取最新的研究进展。

## 4. 结论

GR-2 模型通过其创新的两阶段训练方法（大规模视频预训练和机器人数据微调）、强大的 GPT 风格 Transformer 架构以及对多模态信息（视频、语言、动作）的有效融合，在通用机器人操作领域取得了显著的突破。它不仅能够高效学习大量的桌面任务，还在各种未见过的场景、物体和任务中展现出卓越的泛化能力。此外，GR-2 的性能可随模型规模有效扩展，预示了其未来进一步发展的巨大潜力。端到端分拣实验和 CALVIN 基准测试的SOTA结果进一步证明了 GR-2 在复杂操作和长程任务规划方面的领先水平。其自回归视频生成能力不仅是模型的核心机制，也为理解和改进机器人规划提供了新的视角。

GR-2 的成功为构建具备广泛常识和高度适应性的通用机器人代理奠定了坚实的基础，并为未来机器人技术的发展指明了富有前景的方向。
